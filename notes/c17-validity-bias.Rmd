---
title: 'chapter 17 Validity and Bias'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 17--1 Two Sources of Error in an Estimator 

To recap: in chapter 10 we mentioned the two sources of error in estimation problems:

- **sampling error** or **variability**---the *random* tendency of a result to vary across different samples 
- **bias**---the *systematic* tendency of a result to over/underestimate the true population parameter  

Both of these factors can contribute to the value of an estimate differing from its underlying population value. In module 2 we focused on variability in estimation problems, and we introduced two convergence theorems that describe how variability decreases with sample size.  

Now we will examine *bias* as a source of error. The crucial difference between variability and bias is the former is *random*---owing to random fluctuations---while the latter is *systematic*, and cannot be reduced by increasing the sample size.  

Formally, bias is defined as follows: if $\hat\theta$ is an estimator for the true parameter $\theta$, the *bias* of the estimator is:

$$\Bias[\hat\theta] = \E[\hat\theta] - \theta$$

i.e. bias is when the *expected value* differs from the true value of the parameter being estimated.  

## Reliability vs. Validity 

Two important concepts associated with variability and bias:

- **reliability**---the *consistency* of a result, i.e. the extent to which it produces similar values in different samples
- **validity**---the *accuracy* of a result, i.e. the extent to which it reflects what is actually trying to be measured

Reliability is related to variability---a result is *reliable* if it has low variability, and vice versa. Reliability does not imply validity---e.g. a scale can produce reliable (consistent) estimates of mass, but if incorrectly calibrated, its estimates will be *systematically* incorrect.   

Factors that interfere with validity are known as sources of **bias**. The presence of bias indicates there is something inherently wrong about the *design* of the experiment.  

There are two major kinds of validity in experiments: *internal* validity and *external* validity.    

## External validity

External validity concerns the *generalizability* of a result, i.e. whether it will hold in different samples/settings. For a result to have external validity, it should be *portable*, and it should not depend on in-sample conditions.  

A common threat to external validity is a non-representative sample---this is known as **sampling bias.**    

## Internal validity  

Internal validity concerns the correct identification of causal relationships in an experiment. For an observed result to have internal validity, it must be controlled for by confounding variables, and it must be properly measured/identified. Below are some common threats to internal validity: 

- confounding variables/events that are not controlled for---**omitted variable bias**
- incorrectly identified variables and causal relationships---**specification bias**
- poorly made/calibrated measurements---**measurement error bias**  

In the remainder of this module we will examine sources of bias in experiments and estimation problems.  



\ 

# 17--2 The Bias of an Estimator

Bias is when the expected value of an estimator differs from the true value of the parameter being estimated.  

If $\hat\theta$ is an estimator for the true parameter $\theta$, the bias of the estimator can be written:

$$\Bias[\hat\theta] = \E[\hat\theta] - \theta$$

## Unbiased estimators---the sample mean

From large sample theory we know that the expected value of the sample mean converges to the true mean with a large enough sample: $\E[\bar X] \longrightarrow \mu$. The bias of the sample mean is thus:

$$\Bias[\bar X] = \E[\bar X] - \mu = \mu - \mu = 0$$

In other words, the sample mean is an *unbiased estimator* for the true mean. If you want to estimate the mean of a population, using the sample mean should give you an accurate result, provided the sample is random.    

Not all estimators are unbiased, as the next section will demonstrate.  

## Biased estimators---the sample maximum

Suppose now we want to estimate the *maximum* value of the distribution. Is the sample maximum an unbiased estimator for the true maximum? To answer this question, let $X$ be a RV following a continuous uniform distribution from 0 to 10, i.e. $X \sim \mathcal U(0,10)$. The following code generates 100 random samples of $X$ and plots the histogram:

```{r, echo=FALSE}
set.seed(26)
```

```{r, fig.height=3.5, fig.width=5, fig.align='center'}
X = runif(n = 100, min = 0, max = 10)

ggplot(aes(x = X), data = as.data.frame(X)) + 
  geom_histogram(binwidth = 0.5) + theme_bw()
```

If $\theta$ denotes the true maximum of the population distribution, we know that in this case $\theta = 10$. The sample maximum, $\hat\theta$, is:

```{r}
max(X)
```

If we repeat the process many times, we can construct a sampling distribution of the maximum:

```{r, fig.height=3.5, fig.width=5, fig.align='center'}
thetahat = replicate(n = 1000, max(runif(n = 100, min = 1, max = 10)))

ggplot(aes(x = thetahat), data = as.data.frame(thetahat)) + 
  geom_histogram(bins = 50) + ggtitle('sampling distribution of the maximum') + xlab(TeX('$\\hat{\\theta}$')) + theme_bw()
```

The sampling distribution of $\hat\theta$ is neither bell-shaped, nor centered at the true maximum $\theta$. In fact, based on this sampling distribution, the expected value of the sample maximum is:

```{r}
mean(thetahat)
```

Clearly, the sample maximum underestimates the true maximum, since $\E[\hat\theta] - \theta =$ `r toString(round(mean(thetahat),3))` -10 $< 0$. The bias of the sample maximum in this example is:

```{r}
mean(thetahat) - 10
```

With a bit of calculus it can be shown that:

$$\E[\hat\theta] = \frac{n}{n+1} \theta$$

i.e. the sample maximum is a *biased* estimator for the true maximum, since it will consistently underestimate the true maximum by a factor $\frac{n}{n+1}$. Using this, we can construct a bias-corrected estimator for the maximum:

$$\frac{n+1}{n} \hat\theta$$
where $\hat\theta$ is the sample maximum. 

For the above example, where $n=100$, the bias-corrected estimate of the true maximum is:

```{r}
mean(thetahat)*((100+1)/100)
```

which is clearly a much more *accurate* estimate of the true maximum. 

## Bessel's Correction

Another example of estimator bias arises when estimating the variance of a distribution. We've mentioned before that there are slightly different formulae for the population variance and sample variance:  

$$\sigma^2 = \frac 1n \sum_i^n (X_i - \mu)^2 \hspace{2cm} s^2 = \frac{1}{n-1} \sum_i^n (X_i - \bar X)^2$$

i.e. the sample variance has a degrees-of-freedom adjustment (dividing by $n-1$ rather than $n$). This is known as **Bessel's correction.** It turns out that estimating variance using $\frac 1n$ will consistently *underestimate* the true variance by a factor $\frac{n-1}{n}$. See the proof <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction#Formula">here</a>. 

$$\E \bigg[ \frac 1n \sum_i^n (X_i - \bar X)^2 \bigg] = \frac{n-1}{n} \sigma^2$$

Applying Bessel's correction remedies this bias.   

Note that when $n$ is large, the difference between $\frac 1n$ and $\frac{1}{n-1}$ becomes negligible. The difference between the two formulae is significant only for small samples. This is why we use the $t$-distribution for small samples, since it uses the bias-corrected formula for sample variance, which produces a normal curve with fatter tails. This is also why the $t$-distribution only has one parameter, DoF, since each $n$ produces a slightly different curve.  



\ 

# 17--3 Sampling Bias 

To recap: a **random sample** is one where each observation is selected randomly from the population, and has an equal probability of being selected.  

**Sampling bias** occurs when the observations in a sample are collected in a non-random way, where certain observations in the population have a higher/lower probability of being selected than others. It results in a **biased sample**, which is not representative of the population.  

Below are some examples of sampling bias:

- **response bias** or **self-selection bias**---can occur if study participants self-select (i.e. have control over whether to participate). It could be that the people who voluntarily opt in to the study tend to represent a certain group in the population (e.g. with particularly strong opinions/characteristics), resulting in an overrepresentation of the characteristics of that group. 
    + e.g. using responses from a voluntary survey---it could be that survey respondents tended to have stronger opinions on the issue than nonrespondents, resulting in an overrepresentation of extreme opinions.   
- **referral bias** or **Berkeson's bias**---can occur if the study population is selected from a certain environment that differs from the general population/control group.
    + e.g. in hospital studies, if the admissions rates to the hospital are different for certain ailments (e.g. admission rates of exposed cases and controls differ), the association between exposure and ailment can be distorted. This can result in spurious negative correlations between ailments.    
    + e.g. a person may observe from experience that fast food restaurants in their area that serve good burgers tend to serve bad fries and vice versa; but if s/he doesn't eat anywhere where *both* are bad, the (possibly) large number of restaurants in this category are ignored, which would have weakened or reversed the correlation. 
- **survivorship bias**---can occur if the study only focuses on observations that "survived" after some selection criteria. Ignoring/excluding "failures" can result in optimistic beliefs about the characteristics associated with "successes". 
    + e.g. communicating only with the "survivors" of a fatal incident---can result in underestimating how dangerous the incident really was, since the opinions of the "dead" are not excluded.     
    + e.g. excluding firms that no longer exist in analyses of financial performance---can result in misleading beliefs about what makes a firm successful. 









\ 

---

\ 

\ 




---
title: 'chapter 22 Classification with Logistic Regression'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 22--1 Logistic Regression 

Logistic regression (or logit) is a regression technique for when the outcome variable is binary.   

In logistic regression the goal is to predict $\P(Y=1|X=x)$, where $X$ is the predictor. Although the outcome variable is either 0 or 1, the *predicted* outcome will be a probability, so it can be anything between 0 and 1.   

Complication: to get a linear model, we need to first do a non-linear transformation of $\P(Y=1|X=x)$, using what's known as a “logit” or “log-odds” function.   

If we let $p(x) = \P(Y=1|X=x)$, then the logistic regression model has the following functional form:

$$\ln \bigg(  \frac{p(x_i)}{1-p(x_i)} \bigg) = \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik} + \varepsilon_i$$

where the LHS is the logit function, and the RHS is a linear function of the predictors, just like in OLS regression.  

In logit models the regression coefficients represent a log-odds ratio. If we *exponentiate* the coefficients, we get an odds-ratio, which represents 



\ 

# 22--2 Linguistic Determinism? A Study 

This is a real-world example of a logit model.  

A study by Keith Chen in 2013 tested a “linguistic-savings hypothesis”, whether grammatical differences in languages affect intertemporal choice and economic decision-making. Chen investigated whether Strong FTR languages (i.e. ones that grammatically ‘mark’ the future) cause speakers to discount the future more than Weak FTR languages (i.e. ones that do not grammatically mark the future), and whether this behavioural difference is manifest in individual savings behaviour. His study concluded that the distancing mechanism inherent in Strong FTR languages does indeed significantly lower the probability that an individual will take future-oriented actions. 

First, a quick note on language:   

A Strong FTR language is one that grammatically 'marks' the future (with a word, or a set of words, etc.). English and French are two such examples:  

- English: "it *will* rain tomorrow"  
- French: "il *va* pleuvoir demain" (it will rain tomorrow)

Weak FTR languages do not grammatically mark the future in this manner; often in such languages the present tense form of the verb is used, and the notion of future time is gleaned rather from context than grammatical markers. German and Finnish are two examples:  

- German: "Morgen regnet es" (it rains tomorrow)  
- Finnish: "Huomenna sataa" (tomorrow rains)  

Since saving money is a future-oriented activity, Chen claimed the distancing mechanism in Strong FTR languages causes people to (unwittingly) save less money. In his study, Chen found a significant difference in savings behaviour between speakers of Strong FTR and Weak FTR languages, even when controlling for a number of individual and country variables.  

Chen used a logit model to estimate the probability than an individual will save money, based on whether they speak a Strong or Weak FTR language:

$$\P(\text{save}_{it}) = \frac{\exp(z_{it})}{1 + \exp(z_{it})}$$

where:

$$z_{it} = \beta_{0} + \beta_{1} \text{StrongFTR} + \beta_{2} X_{it} + \beta_{3} X_{t} + \beta_{4} F_{it}^{ex} + \beta_{5} F_{t}^{c}$$













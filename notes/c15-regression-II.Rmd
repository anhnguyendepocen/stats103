---
title: 'chapter 15 Regression Revisited'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

\newcommand{\SSR}{\text{SSR}}

\newcommand{\mhat}[1]{\skew{3}\hat{#1}}

# 15--1 Regression Coefficients as Estimates

In module 1 we introduced linear regression as a technique for modeling linearly related variables. We now have the tools to consider regression models with a little more rigour.   

Recall the general form of a regression model:

$$y = \beta_0 + \beta_1  x_1 + \beta_2 x_2 + ... + \beta_k x_k + \varepsilon$$

where $y$ is the outcome variable, $x_1, ..., x_k$ are a set of $k$ predictor variables, $\beta_1, ..., \beta_k$ are their respective coefficients ($\beta_0$ is the $y$-intercept), and $\varepsilon$ represents the error terms or residuals of the model.  

Recall that the coefficient $\beta$ for a particular variable describes the effect of that variable on the response variable, while holding the other predictors constant (i.e. while *controlling* for the effects of the other predictors).   

Note the symbol $\beta$ is typically refers to the *true* regression coefficient (i.e. the coefficient we would get if we ran the regression on population data). Of course, for a given sample of data, the regression coefficients we compute are only *estimates* for the true coefficients, and like all other estimates they are subject to the whims of variability. We typically use a different symbol, $b$, to represent the *estimated* regression equation:

$$y = b_0 + b_1x_1 + b_2 x_2 + ... + b_k x_k + e$$

where $b_1, ..., b_k$ are the set of *estimated* regression coefficients, and $e$ represents the *estimated* error terms of the model.  

Note since regression models provide a way of determining the the value of $y$ (the response variable) by *using* a predictor variable, $x$, we can use the terminology of expectation to realize the regression equation is simply another way of saying $\E[y | x]$, i.e. the *expected value* of $y$ given $x$. In the case of multiple predictors, we can say $\E[y | x_1, ..., x_k]$.   

## Confidence intervals for regression coefficients

Sample-derived regression coefficients are *point estimates* of the true regression coefficients. In the same way that we construct confidence intervals for means, we can also construct confidence intervals for regression coefficients, as *interval estimates* of the true regression coefficients.  

Consider the simple regression model of temperature on building height from the NYC heatwave data:

```{r}
reg1 = lm(temperature ~ building_height, data = nycheat)
summary(reg1)
```

The estimated coefficient for building height is 0.48. The column `Std. Error` gives the standard error of this estimate. Recall that using the standard error we can construct a 95\% confidence interval on a point estimate, with the lower and upper bounds approximately $2 \; \SE$ below and above the estimate.  

In R you can use the `confint()` function to compute a confidence interval for the coefficients of a regression model:

```{r}
confint(reg1)
```

i.e. the lower bound of the interval is 0.39 and the upper bound is 0.58. To get a 99\% confidence interval:

```{r}
confint(reg1, level = 0.99)
```

## Generating a prediction interval 

After we've specified a regression model (i.e. estimated values for the coefficients), we can use it to predict the value of the response variable for given values of the predictors. The predicted value of a regression model, denoted $\mhat y$, is a *point estimate* of the model's prediction. We can generate an *interval estimate* of the model's prediction using the **residual standard error**.  

First, consider the simple regression model of temperature on albedo, as depicted below, with the regression line overlaid:

```{r, fig.height=3.5, fig.width=5, fig.align='center', echo=FALSE}
ggplot(aes(x = albedo, y = temperature), data = nycheat) +
  geom_point(size=0.5) +
  stat_smooth(method='lm', se = FALSE) +           # overlay regression line
  xlab('albedo (% solar reflectivity)') + ylab('temperature (farenheit)') +
  theme_bw()
```

As it turns out, we can use the CLT to model the error terms of a regression model as *normally distributed* with mean 0 and standard deviation equal to the *residual standard error*. The residual standard error is assumed to be constant (so at any point on the regression line, the error has the same distribution). Below is the regression output for temperature on albedo:

```{r}
reg2 = lm(temperature ~ albedo, data = nycheat)
summary(reg2)
```

The third line from the bottom shows the residual standard error of this model: 3.984. Using this we can generate a 95\% *prediction interval* for the predicted value of temperature given albedo. You can either do this manually (using the standard expression for a confidence interval) or simply use the `predict()` function in R.  

E.g. below is a 95\% prediction interval for the value of temperature when albedo = 10: 

```{r}
predict(reg2, newdata = data.frame(albedo=10), interval = 'predict', level = 0.95)
```

Note `fit` refers to the fitted value of the model (i.e. the point-estimate of the model prediction).  


## Hypothesis tests on regression coefficients


\ 

# The Method of Least Squares 

Although we described the regression coefficients as representing the effect of a predictor on a response variable. We did not show how they are derived. Here we do.   

Regression coefficients are typically computed using the method of least squares, i.e. the coefficients that produce an equation that minimizes the sum of squared errors.  

It turns out, for a simple (bivariate) regression, the coefficient on the predictor is given:

$$b_1 = r_{xy}\frac{s_y}{s_x}$$



\ 

## Bivariate Derivation  

For a given set of sample data on two variables, the regression equation predicting $y$ given $x$ is:

$$y_i = b_0 + b_1 x_i + e_i$$

Our goal is to find $b_0$ and $b_1$ such that the sum of squared residuals (SSR) is minimized. First, we can rearrange the equation above to get the error as a function of $b_0$ and $b_1$:

$$e_i = e_i(b_0,b_1) = y_i - b_0 - b_1 x_i$$

The squared residuals are simply $(e_i)^2 = (y_i - b_0 - b_1 x_i)^2$, and the *sum of squared residuals* is:

$$\SSR(b_0,b_1) = \sum_i^n (y_i - b_0 - b_1 x_i)^2$$

To find $b_0$ for which $\SSR$ is minimum, we set the partial derivative of $\SSR$ with respect to $b_0$ equal to zero:

$$\frac{\partial{\SSR}}{\partial b_0} = -2 \sum_i^n(y_i - b_0 - b_1 x_i) = 0$$

Rearranging:

$$
\begin{aligned}
  \sum_i^n b_0 &= \sum_i^n y_i - \sum_i^n b_1 x_i \\ 
  \therefore \;\; b_0 &= \frac 1n \sum_i^n y_i - \frac 1n \sum_i^n b_1 x_i \\ 
  \therefore \;\; b_0 &= \bar y - b_1 \bar x
\end{aligned}  
$$

where we have used the fact that $\frac 1n \sum_i^n y_i = \bar y$.  

Next, to find $b_1$ for which $\SSR$ is minimum, set the partial derivative of $\SSR$ with respect to $b_1$ equal to zero:

$$\frac{\partial{\SSR}}{\partial b_1} = -2 \sum_i^n x_i (y_i - b_0 - b_1 x_i) = 0$$

Rearranging:

$$
\begin{aligned}
  \sum_i^n b_1 x_i^2 &= \sum_i^n x_i y_i - \sum_i^n x_i b_0 \\ 
  &= \sum_i^n x_i y_i - \sum_i^n x_i (\bar y - b_1 \bar x) \\ 
  &= \sum_i^n x_i y_i - \sum_i^n x_i \bar y - \sum_i^n x_i b_1 \bar x \\ 
\end{aligned}  
$$

Using the fact that $\sum_i^n y_i = N\bar y$ and $\sum_i^n x_i = N \bar x$, and pulling constants out of the sum:

$$
\begin{aligned}
  b_1 \sum_i^n x_i^2 &= \sum_i^n x_i y_i - N \bar x \bar y - b_1 N \bar x^2 \\ 
  \therefore \;\; b_1 &= \frac{\sum_i^n x_i y_i - N \bar x \bar y}{\sum_i^n x_i^2 - N \bar x^2} \\ 
  \therefore \;\; b_1 &= \frac{\sum_i^n (x_i - \bar x)(y_i - \bar y)}{\sum_i^n (x_i - \bar x)^2} = \frac{s_{x,y}}{s_x^2} = r_{xy} \frac{s_y}{s_x}
\end{aligned}
$$

where we have used the fact that $\sum_i^n x_i y_i - N \bar x \bar y = \sum_i^n (x_i - \bar x)(y_i - \bar y)$. Note $s_{x,y}$ here denotes the sample covariance between $x$ and $y$. 



\ 

## Multivariate derivation 




\ 

# The Gauss-Markov Theorem 

Note the variance of the regression coefficient is:

$$\Var[b_1] = \frac{\sigma^2}{\sum_i^n (x_i - \bar x)^2}$$

The Gauss-Markov theorem states the best linear unbiased estimator is given by the OLS estimator. Here best means lowest variance.  







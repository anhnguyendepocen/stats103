---
title: 'chapter 15 A Little Regression Theory'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

\newcommand{\SSR}{\text{SSR}}

\newcommand{\mhat}[1]{\skew{3}\hat{#1}}

--- 

# 15--1 Regression Coefficients as Estimates

In module 1 we introduced linear regression as a technique for modeling linearly related variables. We now have the tools to consider regression models with a little more rigour.   

Recall the general form of a regression model:

$$y = \beta_0 + \beta_1  x_1 + \beta_2 x_2 + ... + \beta_k x_k + \varepsilon$$

where $y$ is the outcome variable, $x_1, ..., x_k$ are a set of $k$ predictor variables, $\beta_1, ..., \beta_k$ are their respective coefficients ($\beta_0$ is the $y$-intercept), and $\varepsilon$ represents the error terms or residuals of the model.  

Recall that the coefficient $\beta$ for a particular variable describes the effect of that variable on the response variable, while holding the other predictors constant (i.e. while *controlling* for the effects of the other predictors).   

Note the symbol $\beta$ is typically refers to the *true* regression coefficient (i.e. the coefficient we would get if we ran the regression on population data). Of course, for a given sample of data, the regression coefficients we compute are only *estimates* for the true coefficients, and like all other estimates they are subject to the whims of variability. We typically use a different symbol, $b$, to represent the *estimated* regression equation:

$$y = b_0 + b_1x_1 + b_2 x_2 + ... + b_k x_k + e$$

where $b_1, ..., b_k$ are the set of *estimated* regression coefficients, and $e$ represents the *estimated* error terms of the model.  

Note since regression models provide a way of determining the the value of $y$ (the response variable) by *using* a predictor variable, $x$, we can use the terminology of expectation to realize the regression equation is simply another way of saying $\E[y | x]$, i.e. the *expected value* of $y$ given $x$. In the case of multiple predictors, we can say $\E[y | x_1, ..., x_k]$.   


\ 

# 15--2 The Method of Least Squares 

The technique most commonly used to fit the regression line is known as the *method of least squares*---this technique involves finding coefficients that minimize the sum of squared errors.     

In a simple (bivariate) regression, it turns out the value for $b_1$ that minimizes the sum of squared errors is given by:

$$b_1 = r_{xy}\frac{s_y}{s_x}$$

where $r_{xy}$ is the sample correlation between $x$ and $y$, and $s_y$ and $s_x$ are the sample standard deviations of $y$ and $x$. The proof for this is below.  

## Bivariate Derivation  

For a given set of sample data on two variables, the regression equation predicting $y$ given $x$ is:

$$y_i = b_0 + b_1 x_i + e_i$$

Our goal is to find $b_0$ and $b_1$ such that the sum of squared residuals (SSR) is minimized. First, rearrange the equation above to get the error as a function of $b_0$ and $b_1$:

$$e_i = e_i(b_0,b_1) = y_i - b_0 - b_1 x_i$$

The squared residuals are simply $(e_i)^2 = (y_i - b_0 - b_1 x_i)^2$, and the *sum of squared residuals* is:

$$\SSR(b_0,b_1) = \sum_i^n (y_i - b_0 - b_1 x_i)^2$$

To find $b_0$ for which $\SSR$ is minimum, set the partial derivative of $\SSR$ with respect to $b_0$ equal to zero:

$$\frac{\partial{\SSR}}{\partial b_0} = -2 \sum_i^n(y_i - b_0 - b_1 x_i) = 0$$

Rearranging, and using the fact that $\frac 1n \sum_i^n y_i = \bar y$:

$$
\begin{aligned}
  \sum_i^n b_0 &= \sum_i^n y_i - \sum_i^n b_1 x_i \\ 
  b_0 &= \frac 1n \sum_i^n y_i - \frac 1n \sum_i^n b_1 x_i \\ 
  b_0 &= \bar y - b_1 \bar x
\end{aligned}  
$$

Next, to find $b_1$ for which $\SSR$ is minimum, set the partial derivative of $\SSR$ with respect to $b_1$ equal to zero:

$$\frac{\partial{\SSR}}{\partial b_1} = -2 \sum_i^n x_i (y_i - b_0 - b_1 x_i) = 0$$

Rearranging:

$$
\begin{aligned}
  \sum_i^n b_1 x_i^2 &= \sum_i^n x_i y_i - \sum_i^n x_i b_0 \\ 
  &= \sum_i^n x_i y_i - \sum_i^n x_i (\bar y - b_1 \bar x) \\ 
  &= \sum_i^n x_i y_i - \sum_i^n x_i \bar y - \sum_i^n x_i b_1 \bar x \\ 
\end{aligned}  
$$

Using the fact that $\sum_i^n y_i = N\bar y$ and $\sum_i^n x_i = N \bar x$, and pulling constants out of the sum:

$$
\begin{aligned}
  b_1 \sum_i^n x_i^2 &= \sum_i^n x_i y_i - N \bar x \bar y - b_1 N \bar x^2 \\ 
  b_1 &= \frac{\sum_i^n x_i y_i - N \bar x \bar y}{\sum_i^n x_i^2 - N \bar x^2} \\ 
  b_1 &= \frac{\sum_i^n (x_i - \bar x)(y_i - \bar y)}{\sum_i^n (x_i - \bar x)^2} = \frac{s_{xy}}{s_x^2} = r_{xy} \frac{s_y}{s_x}
\end{aligned}
$$

where we have used the fact that $\sum_i^n x_i y_i - N \bar x \bar y = \sum_i^n (x_i - \bar x)(y_i - \bar y)$. Note $s_{xy}$ denotes the sample covariance between $x$ and $y$. 

## Multivariate derivation 

`coming soon`






\ 

# 15--3 The Gauss-Markov Theorem 

Note the variance of the regression coefficient is:

$$\Var[b_1] = \frac{\sigma^2}{\sum_i^n (x_i - \bar x)^2}$$

The Gauss-Markov theorem states the best linear unbiased estimator is given by the OLS estimator. Here best means lowest variance.



\ 

# 15--4 The Assumptions of LS Regression 

`coming soon`

## Linearity 

\ 

## No Multicollinearity 

\   

## Mean-Zero Errors

\ 

## Constant-Variance Errors 

\ 

## Normal Errors






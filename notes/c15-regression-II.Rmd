---
title: 'chapter 15 Regression Revisited'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

\newcommand{\SSR}{\text{SSR}}

\newcommand{\mhat}[1]{\skew{3}\hat{#1}}

--- 

# 15--1 Regression Coefficients as Estimates

In module 1 we introduced linear regression as a technique for modeling linearly related variables. We now have the tools to consider regression models with a little more rigour.   

Recall the general form of a regression model:

$$y = \beta_0 + \beta_1  x_1 + \beta_2 x_2 + ... + \beta_k x_k + \varepsilon$$

where $y$ is the outcome variable, $x_1, ..., x_k$ are a set of $k$ predictor variables, $\beta_1, ..., \beta_k$ are their respective coefficients ($\beta_0$ is the $y$-intercept), and $\varepsilon$ represents the error terms or residuals of the model.  

Recall that the coefficient $\beta$ for a particular variable describes the effect of that variable on the response variable, while holding the other predictors constant (i.e. while *controlling* for the effects of the other predictors).   

Note the symbol $\beta$ is typically refers to the *true* regression coefficient (i.e. the coefficient we would get if we ran the regression on population data). Of course, for a given sample of data, the regression coefficients we compute are only *estimates* for the true coefficients, and like all other estimates they are subject to the whims of variability. We typically use a different symbol, $b$, to represent the *estimated* regression equation:

$$y = b_0 + b_1x_1 + b_2 x_2 + ... + b_k x_k + e$$

where $b_1, ..., b_k$ are the set of *estimated* regression coefficients, and $e$ represents the *estimated* error terms of the model.  

Note since regression models provide a way of determining the the value of $y$ (the response variable) by *using* a predictor variable, $x$, we can use the terminology of expectation to realize the regression equation is simply another way of saying $\E[y | x]$, i.e. the *expected value* of $y$ given $x$. In the case of multiple predictors, we can say $\E[y | x_1, ..., x_k]$.   

## Confidence intervals for regression coefficients

Sample-derived regression coefficients are *point estimates* of the true regression coefficients. In the same way that we construct confidence intervals for means, we can also construct confidence intervals for regression coefficients, as *interval estimates* of the true regression coefficients.  

Consider the simple regression model of temperature on building height from the NYC heatwave data:

```{r}
reg1 = lm(temperature ~ building_height, data = nycheat)
summary(reg1)
```

The estimated coefficient for building height is 0.48. The column `Std. Error` gives the standard error of this estimate. Recall that using the standard error we can construct a 95\% confidence interval on a point estimate, with the lower and upper bounds approximately $2 \; \SE$ below and above the estimate.  

In R you can use the `confint()` function to compute a confidence interval for the coefficients of a regression model:

```{r}
confint(reg1)
```

i.e. the lower bound of the interval is 0.39 and the upper bound is 0.58. To get a 99\% confidence interval:

```{r}
confint(reg1, level = 0.99)
```

## Generating a prediction interval 

After we've specified a regression model (i.e. estimated values for the coefficients), we can use it to predict the value of the response variable for given values of the predictors. The predicted value of a regression model, denoted $\mhat y$, is a *point estimate* of the model's prediction. We can generate an *interval estimate* of the model's prediction using the **residual standard error**.  

Consider the simple regression model of temperature on albedo, as depicted below, with the regression line overlaid in blue:

```{r, fig.height=3.5, fig.width=5, fig.align='center', include=FALSE}
ggplot(aes(x = albedo, y = temperature), data = nycheat) +
  geom_point(size=0.5) +
  stat_smooth(method='lm', se = FALSE) +           # overlay regression line
  xlab('albedo (% solar reflectivity)') + ylab('temperature (farenheit)') +
  theme_bw()

ggsave('./pics/c15-pic2.png')
```

```{r, fig.height=3.5, fig.width=5, fig.align='center', echo=FALSE}
knitr::include_graphics('./pics/c15-pic3.png')
```


As it turns out, the CLT allows us to model the error terms of a regression model as *normally distributed* with mean 0 and standard deviation equal to the *residual standard error*. The residual standard error is assumed to be constant (so at any point on the regression line, the error has the same distribution). In the above plot, the distribution of the residuals is depicted in red at two points in the data. Below is the regression output for temperature on albedo:

```{r}
reg2 = lm(temperature ~ albedo, data = nycheat)
summary(reg2)
```

The third line from the bottom shows the residual standard error of this model: 3.984. Using this we can generate a 95\% *prediction interval* for the predicted value of temperature given albedo. You can either do this manually (using the standard expression for a confidence interval) or simply use the `predict()` function in R.  

E.g. below is a 95\% prediction interval for the value of temperature when albedo = 10: 

```{r}
predict(reg2, newdata = data.frame(albedo=10), interval = 'predict', level = 0.95)
```

Note `fit` refers to the fitted value of the model (i.e. the point-estimate of the model prediction).  


## Hypothesis tests on regression coefficients

We can use the hypothesis testing framework to test whether the predictor variables in a given model are really significant. Consider the multiple regression model of temperature, using using vegetation, albedo, building height, and population density as predictors:

```{r}
reg3 = lm(temperature ~ vegetation + albedo + building_height + pop_density, data = nycheat)
summary(reg3)
```

Note that each estimated coefficient also has an associated $p$-value (the column `Pr(>|t|)`). In R (and other software) the regression coefficients are treated as hypothesis tests, with the null hypothesis that the true coefficient is zero, i.e. 

$$H_0: \beta = 0$$ 
$$H_1: \beta \neq 0$$

The $p$-value represents the likelihood of getting a value as extreme as the observed coefficient if the null were true (i.e. if the coefficient were really zero). In the output above, the first three predictors have very low $p$-values, suggesting that the coefficients for those variables are significant. Note $p$-value for population density is larger than the others, at 0.02, and is thus significant only at the 5\% level.  

\ 

# 15--2 The Method of Least Squares 

The technique most commonly used to fit the regression line is known as the *method of least squares*---this technique involves finding coefficients that minimize the sum of squared errors.     

In a simple (bivariate) regression, it turns out the value for $b_1$ that minimizes the sum of squared errors is given by:

$$b_1 = r_{xy}\frac{s_y}{s_x}$$

where $r_{xy}$ is the sample correlation between $x$ and $y$, and $s_y$ and $s_x$ are the sample standard deviations of $y$ and $x$. The proof for this is below.  

## Bivariate Derivation  

For a given set of sample data on two variables, the regression equation predicting $y$ given $x$ is:

$$y_i = b_0 + b_1 x_i + e_i$$

Our goal is to find $b_0$ and $b_1$ such that the sum of squared residuals (SSR) is minimized. First, rearrange the equation above to get the error as a function of $b_0$ and $b_1$:

$$e_i = e_i(b_0,b_1) = y_i - b_0 - b_1 x_i$$

The squared residuals are simply $(e_i)^2 = (y_i - b_0 - b_1 x_i)^2$, and the *sum of squared residuals* is:

$$\SSR(b_0,b_1) = \sum_i^n (y_i - b_0 - b_1 x_i)^2$$

To find $b_0$ for which $\SSR$ is minimum, set the partial derivative of $\SSR$ with respect to $b_0$ equal to zero:

$$\frac{\partial{\SSR}}{\partial b_0} = -2 \sum_i^n(y_i - b_0 - b_1 x_i) = 0$$

Rearranging, and using the fact that $\frac 1n \sum_i^n y_i = \bar y$:

$$
\begin{aligned}
  \sum_i^n b_0 &= \sum_i^n y_i - \sum_i^n b_1 x_i \\ 
  b_0 &= \frac 1n \sum_i^n y_i - \frac 1n \sum_i^n b_1 x_i \\ 
  b_0 &= \bar y - b_1 \bar x
\end{aligned}  
$$

Next, to find $b_1$ for which $\SSR$ is minimum, set the partial derivative of $\SSR$ with respect to $b_1$ equal to zero:

$$\frac{\partial{\SSR}}{\partial b_1} = -2 \sum_i^n x_i (y_i - b_0 - b_1 x_i) = 0$$

Rearranging:

$$
\begin{aligned}
  \sum_i^n b_1 x_i^2 &= \sum_i^n x_i y_i - \sum_i^n x_i b_0 \\ 
  &= \sum_i^n x_i y_i - \sum_i^n x_i (\bar y - b_1 \bar x) \\ 
  &= \sum_i^n x_i y_i - \sum_i^n x_i \bar y - \sum_i^n x_i b_1 \bar x \\ 
\end{aligned}  
$$

Using the fact that $\sum_i^n y_i = N\bar y$ and $\sum_i^n x_i = N \bar x$, and pulling constants out of the sum:

$$
\begin{aligned}
  b_1 \sum_i^n x_i^2 &= \sum_i^n x_i y_i - N \bar x \bar y - b_1 N \bar x^2 \\ 
  b_1 &= \frac{\sum_i^n x_i y_i - N \bar x \bar y}{\sum_i^n x_i^2 - N \bar x^2} \\ 
  b_1 &= \frac{\sum_i^n (x_i - \bar x)(y_i - \bar y)}{\sum_i^n (x_i - \bar x)^2} = \frac{s_{xy}}{s_x^2} = r_{xy} \frac{s_y}{s_x}
\end{aligned}
$$

where we have used the fact that $\sum_i^n x_i y_i - N \bar x \bar y = \sum_i^n (x_i - \bar x)(y_i - \bar y)$. Note $s_{xy}$ denotes the sample covariance between $x$ and $y$. 

## Multivariate derivation 








\ 

# 15--3 The Gauss-Markov Theorem 

Note the variance of the regression coefficient is:

$$\Var[b_1] = \frac{\sigma^2}{\sum_i^n (x_i - \bar x)^2}$$

The Gauss-Markov theorem states the best linear unbiased estimator is given by the OLS estimator. Here best means lowest variance.  







---
title: 'chapter 13 Testing Significance'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 13--1 The Hypothesis Testing Framework

A statistical hypothesis is an *assumption* about one or more population parameters. One of our goals in statistical inference is testing whether a given hypothesis is really valid, or whether it should be rejected.  

In a **hypothesis test** we compare two competing models for what a population really looks like, and use the results of the comparison to decide the fate of a hypothesis. There is a *proposed model*, which represents the assumption we want to test---this is called the *null hypothesis*. The null can be based on anything---the results of a previous experiment, a scientific status quo, or even just a hunch^[Although in general, statistically unfounded hunches are not advisable.] we might have about the population.  

To test the null we compare it to a *competing* model for the population parameter(s), usually in the form of a (new) sample of data. If the competing model yields vastly different results results to those predicted by the null model, we have grounds to reject the null. The **alternative hypothesis** describes the scenario under which the null is not true.  

- **the null hypothesis**, $H_0$: a proposed model for the population parameter(s)
- **the alternative hypothesis**, $H_1$: the scenario under which the null is not true  

There are two possible outcomes from a hypothesis test: either we *reject* the null or we *fail to reject* it---there's no in-between. For this reason hypothesis testing is only appropriate for testing a *well-defined* hypothesis---in other cases estimation and confidence intervals are better tools.  

Note the alternative hypothesis is always stated as a *negation* of the null. Hypothesis tests give a framework for rejecting a given model, but not for uniquely specifying one.  

## A simple example

In the pay gap data, the variable `DiffMeanHourlyPercent`^[i.e. the difference in mean hourly wages between females and males.] has a sample mean $\bar X =$ 12.356\%. Suppose we use this to define a null hypothesis, that the *true* mean hourly wage gap is 12.356\%.   

$$H_0: \mu = 12.356$$
$$H_1: \mu \neq 12.356$$

Suppose that the following year we collect a new sample of data, which for the same variable yields a sample mean $\bar X = 9.42$. We can now use this new evidence to test our initial hypothesis and determine whether it should be rejected.   

To make this determination we must quantify the *probability* of getting the observed statistic^[The observed value is the value proposed by the *competing* model.], if the null hypothesis were really true. If this probability is sufficiently low, there's a good chance the null hypothesis is not true.  

*What constitutes a sufficiently low probability?* The probability threshold below which we *definitively* reject the null is known as the **significance level** ($\alpha$). Conventionally a significance level of $\alpha = 0.05$ is used---i.e. we reject the null if the observed value lies in the most extreme 5\% of values under the null distribution. Sometimes a significance level of $\alpha = 0.01$ is also used.    

The **rejection region** of a test is the range of values for which which we reject the null. The size of the rejection region is determined by the significance level we decide on. Below are two plots of the distribution of $\bar X$ under the null hypothesis (as defined above), with rejection regions for $\alpha = 0.05$ and $\alpha = 0.01$ shown: 


```{r, echo=FALSE, warning=FALSE, fig.width = 8, fig.height=3}
Xbar = mean(paygap$DiffMeanHourlyPercent)   #sample mean
s = sd(paygap$DiffMeanHourlyPercent)   #sample s.d.
n = nrow(paygap)   #sample size
SE = s / sqrt(n)   #standard error

x = seq(-4, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)

df = data.frame(x = x, y = y)

breaks = round(seq(Xbar-4*SE, Xbar+4*SE, SE),2)

alpha5 = 0.05
alpha1 = 0.01

Z_5 = qnorm(1-alpha5/2)
Z_1 = qnorm(1-alpha1/2)

plot1 = ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('rejection region for $\\alpha = 0.05$')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') +
  geom_text(label = 'p = 0.025', x = 8.8, y = -0.005, size = 2.5, color = 'violetred') +
  geom_text(label = 'p = 0.025', x = 15.7, y = -0.005, size = 2.5, color = 'violetred') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'violetred') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'violetred') +
  theme_bw()

plot2 = ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('rejection region for $\\alpha = 0.01$')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_1*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_1*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = Xbar - Z_1*SE, xend = Xbar - Z_1*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_1*SE, xend = Xbar + Z_1*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') +
  geom_text(label = 'p = 0.005', x = 8.4, y = -0.005, size = 2.5, color = 'violetred') +
  geom_text(label = 'p = 0.005', x = 16.1, y = -0.005, size = 2.5, color = 'violetred') +
  geom_text(label = 'critical \n value', x = Xbar - Z_1*SE, y = 0.12, size = 3, color = 'violetred') +
  geom_text(label = 'critical \n value', x = Xbar + Z_1*SE, y = 0.12, size = 3, color = 'violetred') +
  theme_bw()

grid.arrange(plot1, plot2, ncol = 2)
  
  
  
  # geom_text(label = TeX('$-3 \\frac{ s }{ \\sqrt{n} }$'), x = Xbar-3*SE, y = 0.025, size=3, color = 'darkgrey') +
  # geom_text(label = TeX('$-2 \\frac{ s }{ \\sqrt{n} }$'), x = Xbar-2*SE, y = 0.025, size=3, color = 'darkgrey') +
  # geom_text(label = TeX('$-1 \\frac{ s }{ \\sqrt{n} }$'), x = Xbar-1*SE, y = 0.025, size=3, color = 'darkgrey') +
  # geom_text(label = TeX('$\\bar{X}$'), x = Xbar, y = 0.025, size=3, color = 'darkgrey') +
  # geom_text(label = TeX('$+1 \\frac{ s }{ \\sqrt{n} }$'), x = Xbar+1*SE, y = 0.025, size=3, color = 'darkgrey') +
  # geom_text(label = TeX('$+2 \\frac{ s }{ \\sqrt{n} }$'), x = Xbar+2*SE, y = 0.025, size=3, color = 'darkgrey') +
  # geom_text(label = TeX('$+3 \\frac{ s }{ \\sqrt{n} }$'), x = Xbar+3*SE, y = 0.025, size=3, color = 'darkgrey') +
  # theme_bw()
```

The total area occupied by the rejection region is simply the significance level. The **critical values** are the bounds of the rejection region. If $\alpha = 0.05$, the critical values are the 2.5th and 97.5th percentiles of the distribution (just like the bounds of a 95\% confidence interval). You can compute these using `qnorm()` or `qt()`, as demonstrated last chapter, or you can simply compute a 95\% confidence interval for the mean:

```{r, echo=FALSE}
confidence_interval <- function(data, conflevel) {
  xbar <- mean(data)          # sample mean 
  SE <- sd(data) / sqrt(n)    # standard error
  n <- length(data)           # sample size 
  alpha <- 1 - conflevel      # alpha
  
  lb <- xbar + qt(alpha/2, df = n-1) * SE    # lower bound
  ub <- xbar + qt(1-alpha/2, df = n-1) * SE  # upper bound
  
  cat(paste(c('sample mean =', round(xbar,3), '\n', 
              conflevel*100, '% confidence interval:', '\n', 
              'lower bound =', round(lb,3), '\n', 
              'upper bound =', round(ub,3))))
}
```

```{r}
confidence_interval(paygap$DiffMeanHourlyPercent, 0.95)
```

Thus the rejection region in this test is $\bar X < 9.799$ \& $\bar X > 14.913$.  

A test is **statistically significant** if the observed value falls in the rejection region. In this example, where the observed value is $\bar X = 9.42$ and $\alpha = 0.05$:

```{r, echo=FALSE, warning=FALSE, fig.width=5, fig.height=3.5, fig.align='center'}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('significance test with $\\alpha = 0.05$')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = 9.42, xend = 9.42, y = 0, yend = 0.25, color = 'black', linetype = 'dotted') + 
  geom_text(label = 'observed \n value \n = 9.42', x = 9.42, y = 0.27, size = 3, color = 'black') +
  geom_text(label = 'critical \n value \n = 9.80', x = Xbar - Z_5*SE, y = 0.13, size = 3, color = 'violetred') +
  geom_text(label = 'critical \n value \n = 14.91', x = Xbar + Z_5*SE, y = 0.13, size = 3, color = 'violetred') +
  theme_bw()
```

i.e. with $\alpha = 0.05$, the observed value clearly falls in the rejection region. We can thus reject the null, and conclude that true mean hourly wage gap is not $12.356$, at the 5\% level.    

Note if we had used a significance level of $\alpha = 0.01$, we would have reached a different conclusion: 

```{r, echo=FALSE, warning=FALSE, fig.width=5, fig.height=3.5, fig.align='center'}
ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('significance test with $\\alpha = 0.01$')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_1*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_1*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = Xbar - Z_1*SE, xend = Xbar - Z_1*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_1*SE, xend = Xbar + Z_1*SE, y = 0, yend = 0.1, color = 'violetred', linetype = 'dotted') +
  geom_segment(x = 9.42, xend = 9.42, y = 0, yend = 0.25, color = 'black', linetype = 'dotted') + 
  geom_text(label = 'observed \n value \n = 9.42', x = 9.42, y = 0.27, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar - Z_1*SE, y = 0.12, size = 3, color = 'violetred') +
  geom_text(label = 'critical \n value', x = Xbar + Z_1*SE, y = 0.12, size = 3, color = 'violetred') +
  theme_bw()
```

i.e. with $\alpha = 0.01$, the observed value does *not* fall in the rejection region, and thus we cannot reject the null. The choice of significance level can make or break the fate of a hypothesis.  

## The p-value of a test

Another way to conduct a hypothesis test is by looking at $p$-values.  

The **p-value** of a test is the probability of getting a result *at least as extreme* as the observed value, under the null hypothesis.  

This may sound like a cumbersone definition, but it's easy to visualize. Recall in this example the observed value is $\bar X = 9.42$. The probability of getting a value at least as extreme as $\bar X = 9.42$ is the following region:

```{r, echo=FALSE, warning=FALSE, fig.width=5, fig.height=3.5, fig.align='center'}
p = pnorm(9.42, mean = Xbar, sd = SE)
Z = -qnorm(p)

ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('the p-value')) +
  geom_area(aes(x = ifelse(x < Xbar - Z*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = 9.42, xend = 9.42, y = 0, yend = 0.25, color = 'black', linetype = 'dotted') + 
  geom_text(label = 'observed \n value \n = 9.42', x = 9.42, y = 0.27, size = 3, color = 'black') +
  geom_text(label = 'p = 0.0116', x = 8.8, y = -0.005, size = 3, color = 'violetred') + 
  geom_text(label = 'p = 0.0116', x = 15.7, y = -0.005, size = 3, color = 'violetred') + 
  theme_bw()
```

Each region has a probability of 0.0116, which means the total probability of getting a value at least as extreme as the observed value is 0.0233. Thus the $p$-value of this test is 0.0233.  

The result is statistically significant if the $p$-value is smaller than the significance level of the test. If we had used $\alpha = 0.05$, the result would have been statistically significant, and we would have rejected the null; but if we used $\alpha = 0.01$, we couldn't have rejected the null. Note this method is equivalent to the previous one (i.e. computing the rejection region and seeing whether it contains the observed value).  

## A workflow 

To summarize: below are the basic steps to follow when conducting a hypothesis test: 

- state the null and alternative hypotheses. The null is the assumption you are testing---it's a *proposed* model for one or more population parameters. The alternative hypothesis describes the scenario where the null is not true. 
- choose a significance level, $\alpha$, for the test---this is the probability threshold below which you will *definitively* reject the null. Common levels are $\alpha = 0.05$ and $\alpha = 0.01$.  

Then, either: 

- determine the *rejection region* of the test---a range of values that would cause you to reject the null, if the observed value was seen to lie in this range. The bounds of the rejection region are determined by the significance level.  
- state the observed value---the sample statistic from the new (competing) data 
- determine whether to reject the null hypothesis based on whether the observed value lies in the rejection region or not

or: 

- calculate the $p$-value of the test---the probability of getting a value at least as extreme as the observed value
- reject the null if the $p$-value is smaller than the significance level



\ 

# 13--2  Errors and Power

In a hypothesis test, the null is assumed until there is strong evidence to suggest we should reject it. Of course, the outcome of a single hypothesis test does not necessarily lead to the correct result---the $p$-value (or whatever rubric you use) only represents a *likelihood*, and there's always a chance the conclusion we draw from a given test is the *wrong* conclusion, even if the likelihood is very low.   

## Type I and Type II Errors

There are two kinds of error we can make in a hypothesis test:  

- **Type I Error:** rejecting the null, when the null is *actually* true. The probability of type I error is no more than the significance level of a test.^[Can you see why?]
- **Type II Error:** failing to reject the null, when the null *actually* is false.  

```{r, echo=FALSE}
hyptests = as.table(rbind(c('$\\checkmark$','type I error'), c('type II error','$\\checkmark$')))
dimnames(hyptests) = list(' ' = c('$H_0$ true','$H_0$ false'), 
                           ' ' = c('retain null','reject null'))

kable(hyptests)
```

Usually the foremost goal in hypothesis testing is ensuring the probability of type I error is low.^[Since most hypothesis tests are out to *disprove* something, it would be particularly terrible if you rejected a null that was actually true.] Since the type I error is determined by the significance level, you must try to choose a significance level that is appropriate for the context---e.g. if type I errors are particularly dangerous, you should use a small significance level (e.g. $\alpha = 0.01$ or $\alpha = 0.005$), to minimize your chance of rejecting the null when it's actually true.  

Naturally, decreasing the probability of type I error increases the probability of type II error, and vice versa. E.g.---if a judge decided to reduce the number of false convictions by increasing the burden of proof required to demonstrate guilt, this would also inevitably result in more *actual* criminals getting away with their torts.  

The two plots below should help you visualize the type I and type II error. Using the same example as above, the left figure shows the type I error if the true mean wage gap is really 12.36 (i.e. $H_0$ true). The right figure shows the type II error if the true mean wage gap is really 8 (i.e. $H_0$ false). The true distribution is overlaid in pink:  

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height=3.5, fig.fullwidth = TRUE}
Xbar = mean(paygap$DiffMeanHourlyPercent)   #sample mean
s = sd(paygap$DiffMeanHourlyPercent)   #sample s.d.
n = nrow(paygap)   #sample size
SE = s / sqrt(n)   #standard error
mu = 8

x = seq(-10, 4, length = 1000) * SE + Xbar
y = dnorm(x, Xbar, SE)
y1 = dnorm(x, mu, SE)

df = data.frame(x = x, y = y)
df1 = data.frame(x = x, y = y1)

breaks = round(seq(Xbar-8*SE, Xbar+4*SE, SE),2)

alpha5 = 0.05
alpha1 = 0.01

Z_5 = qnorm(1-alpha5/2)
Z_1 = qnorm(1-alpha1/2)

plot1 = ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-4*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('Type I Error if $\\mu = 12.36$ ($H_0$ true)')) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_area(aes(x = ifelse(x > Xbar + Z_5*SE, x, 0)), fill='black', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$H_0: \\; \\mu = 12.36$'), x = 13, y = 0.05, size = 2.5, color = 'black') +
  geom_text(label = 'Type I Error', x = 8.8, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'Type I Error', x = 15.7, y = -0.005, size = 2.5, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') +
  theme_bw()

plot2 = ggplot(data = df1, mapping = aes(x = x, y = y)) +
  geom_line(color = 'violetred') +
  geom_line(data = df, aes(x = x, y = y), color = 'black') +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-8*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('Type II Error if $\\mu = 8$ ($H_0$ false)')) +
  geom_area(aes(x = ifelse(x > Xbar - Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  #geom_area(aes(x = ifelse(x > Xbar + Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_text(label = 'Type II Error', x = 11, y = -0.005, size = 2.5, color = 'violetred') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$H_0: \\; \\mu = 12.36$'), x = 13, y = 0.05, size = 2.5, color = 'black') +
  geom_segment(x = mu, xend = mu, y = 0, yend = 0.5, color = 'violetred', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu = 12.36$'), x = 7.5, y = 0.05, size = 2.5, color = 'violetred') +
  theme_bw()

grid.arrange(plot1, plot2, ncol = 2)
```

Note that the type I error is simply the rejection region of the test. From the figure above you should be able to see why reducing the type I error must also increase the type II error.  

## The power of a test

Another useful concept in hypothsis testing is *power*---this is the probability of *correctly* rejecting a false null. Note that power is the complement of type II error, i.e. Power = 1 - T2E. The secondary goal in hypothesis testing, after choosing the test with the smallest type I error, is to choose the test with maximum power---typically we want power to be greater than 0.8 (i.e. the probability of type II error to be smaller than 0.2).   

Using the same example as above, the power of the test can be visualized as the following region:

```{r, echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=5, fig.align='center'}
ggplot(data = df1, mapping = aes(x = x, y = y)) +
  geom_line(color = 'violetred') +
  geom_line(data = df, aes(x = x, y = y), color = 'black') +
  scale_x_continuous(breaks = breaks,
                     limits = c(Xbar-8*SE, Xbar+4*SE)) +
  ylab('probability') + xlab(TeX('$\\bar{X}$')) +
  ggtitle(TeX('Power if $\\mu = 8$ ($H_0$ false)')) +
  geom_area(aes(x = ifelse(x > Xbar - Z_5*SE, x, 0)), fill='violetred', alpha=0.4) +
  geom_area(aes(x = ifelse(x < Xbar - Z_5*SE, x, 0)), fill='blue', alpha=0.4) +
  geom_segment(x = Xbar - Z_5*SE, xend = Xbar - Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') + 
  geom_segment(x = Xbar + Z_5*SE, xend = Xbar + Z_5*SE, y = 0, yend = 0.1, color = 'black', linetype = 'dotted') +
  geom_text(label = 'Type II Error', x = 11, y = -0.005, size = 2.5, color = 'violetred') +
  geom_text(label = 'Power', x = 7.1, y = -0.005, size = 2.5, color = 'blue') +
  geom_text(label = 'critical \n value', x = Xbar - Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_text(label = 'critical \n value', x = Xbar + Z_5*SE, y = 0.12, size = 3, color = 'black') +
  geom_segment(x = Xbar, xend = Xbar, y = 0, yend = 0.5, color = 'black', linetype = 'dotted') +
  geom_text(label = TeX('$H_0: \\; \\mu = 12.36$'), x = 13, y = 0.05, size = 2.5, color = 'black') +
  geom_segment(x = mu, xend = mu, y = 0, yend = 0.5, color = 'violetred', linetype = 'dotted') +
  geom_text(label = TeX('$\\mu = 12.36$'), x = 7.5, y = 0.05, size = 2.5, color = 'violetred') +
  theme_bw()
```

The further away the true mean is from the proposed mean, the higher the power of the test. To see this mathematically, we'll consider something known as the **power function**, $\beta$,^[Not to be confused with the regression coefficient, which is also denoted $\beta$.] which describes the power of a test as a function of $\mu$, i.e. $\beta = \beta(\mu)$. Since power is the probability of correctly rejecting the null, the power function for a two-tailed test is simply:

$$\beta(\mu) = \P(\bar X > c) + \P(X < -c)$$

where $c = z_{(1-\alpha/2)}$. In full, this can be expressed:

$$
\begin{aligned}
  \beta(\mu) &= \P(\bar X > c) + \P(\bar X <  -c)\\ 
  &= \P \bigg( \frac{\sqrt n (\bar X - \mu)}{\sigma} > \frac{\sqrt n (c - \mu)}{\sigma} \bigg) + \P \bigg( \frac{\sqrt n (\bar X - \mu)}{\sigma} < \frac{\sqrt n (-c - \mu)}{\sigma} \bigg)\\ 
  &= \P \bigg( Z >  \frac{\sqrt n (c - \mu)}{\sigma} \bigg) + \bigg( Z <  \frac{\sqrt n (-c - \mu)}{\sigma} \bigg) \\ 
  &= 1 -\P \bigg( Z < \frac{\sqrt n (c - \mu)}{\sigma} \bigg) + \bigg( Z <  \frac{\sqrt n (-c - \mu)}{\sigma} \bigg) \\ 
  &= 1 - \Phi \bigg(  \frac{\sqrt n (c - \mu)}{\sigma} \bigg) + \Phi \bigg(  \frac{\sqrt n (-c - \mu)}{\sigma} \bigg)
\end{aligned}
$$

where $\Phi$ is the cdf of the normal distribution. Below is a plot of the power as a function of the true mean, using the example above:

Of course, in a two-tailed hypothesis test, the true mean could be above or below the proposed mean, so the power function for a two-tailed test looks as follows:

```{r, echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=5, fig.align='center'}
c_null = qnorm(0.975)

power = function(mu) {
  1 - pnorm(c_null - (10*(mu)/16)) + pnorm(-c_null - (10*(mu)/16))
}

#power(12.36)

xrange = data.frame(mu = seq(from = -8, to = 8, length = 1000))

ggplot(xrange, aes(mu)) + 
  stat_function(fun = power) +
  xlab(TeX('$\\mu$')) + ylab(TeX('$\\beta(\\mu)$')) +
  ggtitle('power as a function of true mean (two-tailed)') +
  geom_segment(x = -10, xend = 0, y = 0.05, yend = 0.05, color  = 'violetred', linetype = 'dotted') +
  geom_text(x = -5, y = 0.1, label = TeX('$\\alpha = 0.05$'), color = 'violetred') + 
  theme_light()
```

We can also introduce another concept: the **size** of a test, which is the largest probability of rejecting $H_0$ when $H_0$ is true. This is simply the value of the power function at the true mean is equal to the null mean, which in this case is $\beta(0) = 0.05$.  



\ 

# 13--3 Effect Size and Practical Significance 

There is a major issue in hypothesis testing that arises when using large samples. In a two-tailed test the critical values (the bounds of the rejection region) are given by $\mu \pm c \cdot \frac{s}{\sqrt n}$, where $\mu$ is the value of the true mean under the null. Note how the critical values have a dependency on $n$ $n$---specifically, that as $n$ becomes larger, the distance between the proposed null and the critical values becomes smaller. This means that for very large $n$, we will reject the null for very small deviations of the observed value from the null---even if the deviations are so small that they have no *practical significance*.   

To illustrate this, suppose we are testing the average height in a population, and the null hypothesis is $\mu = 178$ cm. Suppose we have sample data with $\bar X = 179$ cm, $s = 23.5$ cm, and $n = 100$. If we were to conduct the hypothesis test that $\mu \neq 178$ cm at the $\alpha = 0.05$ level, with $c = Z_{(1-\alpha/2)} = 1.96$, we would have a rejection  region as follows:  

$$\Bigg\{ 178 - 1.96 \cdot \frac{23.5}{\sqrt{100}} \;\; , \;\; 178 + 1.96 \cdot \frac{23.5}{\sqrt{100}} \Bigg\}$$
$$\Longrightarrow \;\;\; \{ 173.39 \; \text{cm},182.61 \; \text{cm} \}$$

i.e. we would reject the null if we observed a value more extreme than the two critical values given. Since our observed value is 179 cm, we do not reject the null, which is an appropriate result since the difference between 178 cm and 179 cm is trivial in the context of people's heights. But if we do the same test with a sample size $n= 10,000$, the rejection region would be:

$$\Bigg\{ 178 - 1.96 \cdot \frac{23.5}{\sqrt{10000}} \;\; , \;\; 178 + 1.96 \cdot \frac{23.5}{\sqrt{10000}} \Bigg\}$$

$$\Longrightarrow \;\;\; \{ 177.54 \; \text{cm},178.46 \; \text{cm} \}$$

i.e. with $n = 10,000$ we would reject our observed value of 179 cm, even though in reality such a small difference doesn't really warrant concluding that the true mean height is not 178 cm. This example illustrates the fact that with large samples, results that are not *practically* significant can still be considered *statistically* significant, and the larger the sample size, the more this is going to happen.  
 






\ 

# 13--4 One-Tailed Hypothesis Tests

So far we have demonstrated only *two-tailed* hypothesis tests, since we assumed the true value could be either above or below the proposed value. This is why, when constructing the rejection region, we split the area equally between the lowest *and* highest extremes (i.e. tails) of the distribution. Most tests are two-tailed, since generally we don't know the direction of the true value relative to our proposed value.  

But there are select instances when we know (or assume) that the true value could *only* be higher (or lower) than the proposed value---in these contexts it may be more optimal to consider only the upper (or lower) tail of the distribution when conducting the test.  

*Example:* suppose someone tells you the temperature of a room is 0 Kelvin (i.e. absolute zero; there is no possible temperature below this value), and you want to test this claim. In this case you can specify a directionality for the alternative hypothesis, since the true mean temperature cannot be below the proposed value:

$$H_0: \mu = 0$$
$$H_1: \mu > 0$$

To conduct this one-tailed test at the 5\% significance level, the rejection region comprises only the uppermost extreme 5\% of values under the distribution:

```{r, fig.height=3.5, fig.width=5, fig.align='center'}
x = seq(-4, 4, length = 1000)
y = dnorm(x, mean = 0, sd = 1)

df = data.frame(x = x, y = y)

ggplot(data = df, mapping = aes(x = x, y = y)) +
  geom_line() +
  xlab('temperature (K)') + 
  ylab('probability') + 
  ggtitle(TeX('$\\alpha = 0.05$ rejection region for one-tailed test for an increase')) + 
  theme_classic() +
  ylim(-0.05,0.42) +
  theme(axis.text.x=element_blank()) +
  theme(axis.ticks.x=element_blank()) +
  geom_area(aes(x = ifelse(x > 1.64, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = 1.64, xend = 1.64, y = 0, yend = 0.3, linetype = 'dotted', color = 'violetred') +
  geom_text(label = 'critical value \n = 95th percentile', x = 1.64, y = 0.35,  color = 'violetred') 
```

i.e. with $\alpha = 0.05$ the critical value for a one-tailed test is the 95th percentile of the distribution (in a two-tailed test with $\alpha = 0.05$, the critical values are the 2.5th and 97.5th percentiles).   



\ 

# 13--5 Testing for a Difference in Means

A score ages ago was 290. The score now is 285. Is this a statistically significant change, or is it just chance variation? 

You can use the z-test. Recall the z-statistic is calculated as follows:

$$Z = \frac{\bar X - \mu}{SE}$$

where $SE = \frac{s}{\sqrt n}$, where $s$ is the sample standard deviation. Once you compute a test statistic, you can calculate its probability, which will give you a result.  

The easiest way to answer this problem is to think of the two samples as two independent RVs. You can then create a new RV that is a linear combination of both. E.g. let X be the RV for the difference between the two samples, i.e. X = W - V. 

The expected value of X is: $E[X] = 0$ since the null is that there is no difference. The observed value is W-V=5. 

Recall the formula for linear combinations of Variance: $Var[W-V] = Var[W] + Var[V] = \frac{s_v^2}{n} + \frac{s_w^2}{n}$. Thus the standard error is:

$$\SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$$




\ 

--- 

\ 

\ 



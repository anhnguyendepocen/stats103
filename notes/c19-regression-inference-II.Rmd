---
title: 'chapter 19 Inference from Regression Models, Part II'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

\newcommand{\SSR}{\text{SSR}}

\newcommand{\mhat}[1]{\skew{3}\hat{#1}}

\renewcommand{\v}[1]{{\boldsymbol #1}}
\renewcommand{\dv}[1]{\dot{\boldsymbol{#1}}}
\newcommand{\ddv}[1]{\ddot{\boldsymbol{#1}}}
\newcommand{\hv}[1]{\hat{\boldsymbol{#1}}}
\newcommand{\m}[1]{[ #1 ]}
\renewcommand{\t}[1]{\widetilde{\boldsymbol{#1}}}
\newcommand{\bfit}[1]{\textbf{\textit{#1}}}

---

# 19--1 Regression Validity

Now we'll examine validity and sources of bias in regression models.  

*__External Validity__*---that the observed causal effect holds true in other settings; that it depends on non-modeled conditions. Questions to ask: is the sample representative? Are the assumptions portable? 

*__Internal Validity__*---that the observed causal effect is properly identified; that the assumptions of least squares regression are met.  

The most important assumption for regression validity is that the errors are uncorrelated with the predictors:

$$\E[\v \varepsilon | \v X] = 0$$

Violations of this assumption will cause LS estimates to become biased. To see this, recall that the regression function $\v y = \v X \v \beta$ can be expressed: 

$$
\begin{aligned}
  \E[\v y | \v X] &= \E[\v X \v \beta + \v \varepsilon | \v X] \\ 
  &= \v X \v \beta + \v \varepsilon | \v X \\ 
  &= \v X \v \beta 
\end{aligned}
$$

i.e. in order for $\E[\v y | \v X] = \v X \v \beta$, it's necessary that $\E[\v \varepsilon | \v X] = 0$, otherwise the estimates will be biased.  

Common sources of bias that cause $\E[\v \varepsilon | \v X] \neq 0$:

- omitted variable bias
- specification bias (nonlinear relationships)
- measurement error bias (noise)
- sample selection bias (sample not representative)
- simultaneity bias ($X$ and $Y$ cause each other)



\ 

# 19--2 Omitted Variable Bias 

**The problem:** correlation between omitted variable(s) and observed predictors makes LS estimates biased.  

E.g. if the *true* model is:

$$\v y = \beta_0 + \beta_1 \v x_1 + \beta_2 \v x_2 + \v \varepsilon$$

but instead we use the following model for $y$:

$$\v y = \beta_0 + \beta_1 \v x_1 + \tilde {\v \varepsilon}$$

i.e. in our model we *omit* the variable $\v x_2$. The error term in our flawed model is $\tilde {\v \varepsilon} = \beta_2 \v x_2 + \v \varepsilon$, i.e. it comprises the error of the true model *and* the effect of omitted variable(s). The LS estimator for $\beta_1$ in our model would be:

$$\v b_1 = (\v x_1^T \v x_1)^{-1} \v y = (\v x_1^T \v x_1)^{-1} \v x_1^T \v x_2 \beta_2 + (\v x_1^T \v x_1)^{-1} \v x_1^T \v \varepsilon$$

and

$$\E[b_1 | \v X] = \beta_1 + (\v x_1^T \v x_1)^{-1} \v x_1^T \v x_2 \beta_2 $$

where the second term on the right represents the bias in the estimate of $b_1$ as a result of omitting $x_2$. Note this term becomes zero if $\v x_1$ and $\v x_2$ are uncorrelated (this would mean $\v x_1^T \v x_2 = 0$). 

**The solution:** include any variables that are correlated with the predictors. These are confounding variables that should be controlled for.  

`R example: coming soon`



\ 

# 19--3 Specification Bias 

**The problem:** there are nonlinear relationships (i.e. the functional form of the model is misspecified; violation of the linearity assumption).  

The linearity assumption requires that each predictor is linear *in parameter* to the response. Sometimes nonlinear relationships can be transformed to become linear, e.g. by taking logs of variables. 

**Solutions:**

- include nonlinear terms (logs or polynomials)
- include interaction terms (if the issue is that $\beta$ varies)
- do model selection to avoid overfitting 

`R example: coming soon`



\ 

# 19--4 Measurement Error Bias

**The problem:** variables are measured with noise---this dampens LS estimates. 

E.g. 

$$\text{truth:} \;\;\; \v y = \beta_0 +  \beta_1 \v x + \v \varepsilon$$
$$\text{data:} \;\;\; \v y = \beta_0 + \beta_1 \v x^* + \v \varepsilon^*$$

where $\v x^*$ represents a noisy measurement of $\v x$. Noise could arise because of recording errors in data entry, rounding errors, etc.  

Below: "true" data is on the left, "noisy" data is on the right---note how noise dampens the estimated slope.  

```{r, echo=FALSE, out.width=400, out.height=200}
knitr::include_graphics("./pics/c20-pic1.png")
```

**Proof:** 

Measurement error can be expressed as follows:

$$x_i^* = x_i + u_i$$

where $u_i$ (noise) is independent of $x_i$ and $\varepsilon_i$ (since it's just noise). 

$$
\begin{aligned}
  y &= \beta_0 + \beta_1 x_i^* + \beta_1 u_i + \varepsilon_i \\ 
  &= \beta_1 + \beta_1 x_i^* + v_i
\end{aligned}
$$

where $v_i$ is the modified error term, $v_i = \beta_1 u_i + \varepsilon_i$. Under this model the LS estimate for $\beta_1$ is:

$$
\begin{aligned}
  b_1 &= \frac{\Cov[x^*, y]}{\Var[x^*]} \\ 
  &= \frac{\Cov[x + u \; , \; \beta_0 + \beta_1 x + \varepsilon]}{\Var[x + u]} \\ 
  &= \frac{\Cov[x \; , \; \beta_0 + \beta_1 x + \varepsilon] + \Cov[u \; , \; \beta_0 + \beta_1 x + \varepsilon]}{\Var[x + u]} \\ 
  &= \frac{\beta_1 \Cov[x,x] + 0}{\Var[x+u]} \\ 
  &= \beta_1 \cdot \frac{\sigma_x^2}{\sigma_x^2 + \sigma_u^2} \\ 
  &= \beta_1 \cdot \text{attenuation}
\end{aligned}  
$$

i.e. the LS estimate converges to the true coefficient multiplied by an attenuation term (which is between 0 and 1). Hence---noise *dampens* the LS estimate (but preserves its sign). 
 
Note:

$$\frac{\sigma_x^2}{\sigma_x^2 + \sigma_u^2} = \frac{1}{1+\sigma_u^2 / \sigma_x^2}$$

The ratio $\frac{\sigma_x^2}{\sigma_u^2}$ is called the **signal to noise ratio**. 

The larger the signal/noise ratio, the smaller the bias.  

**Solution:** use a <a href="https://en.wikipedia.org/wiki/Errors-in-variables_models">measurement error model</a>.  



\ 

# 19--5 Simultaneity Bias 

**The problem:** $x$ and $y$ cause each other. This creates a simultaneous equations model:

$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$$
$$x_i = \gamma_0 + \gamma_1 y_i + \eta_i$$

The LS setimator becomes a weighted average of two components:

- $\beta_1$: the effect of $x$ on $y$
- $\frac{1}{\gamma_1}$: the feedback loop from $y$ to $x$ back to $y$

This is known as simultaneity bias.  

**Proof:** `coming soon` 

 
 
 
\ 

--- 

\ 

\ 







---
title: 'chapter 16 Inference from Regression Models'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 16--1 Confidence intervals for regression coefficients

Sample-derived regression coefficients are *point estimates* of the true regression coefficients. In the same way that we construct confidence intervals for means, we can also construct confidence intervals for regression coefficients, as *interval estimates* of the true regression coefficients.  

Consider the simple regression model of temperature on building height from the NYC heatwave data:

```{r}
reg1 = lm(temperature ~ building_height, data = nycheat)
summary(reg1)
```

The estimated coefficient for building height is 0.48. The column `Std. Error` gives the standard error of this estimate. Recall that using the standard error we can construct a 95\% confidence interval on a point estimate, with the lower and upper bounds approximately $2 \; \SE$ below and above the estimate.  

In R you can use the `confint()` function to compute a confidence interval for the coefficients of a regression model:

```{r}
confint(reg1)
```

i.e. the lower bound of the interval is 0.39 and the upper bound is 0.58. To get a 99\% confidence interval:

```{r}
confint(reg1, level = 0.99)
```

## Generating a prediction interval 

After we've specified a regression model (i.e. estimated values for the coefficients), we can use it to predict the value of the response variable for given values of the predictors. The predicted value of a regression model, denoted $\mhat y$, is a *point estimate* of the model's prediction. We can generate an *interval estimate* of the model's prediction using the **residual standard error**.  

Consider the simple regression model of temperature on albedo, as depicted below, with the regression line overlaid in blue:

```{r, fig.height=3.5, fig.width=5, fig.align='center', include=FALSE}
ggplot(aes(x = albedo, y = temperature), data = nycheat) +
  geom_point(size=0.5) +
  stat_smooth(method='lm', se = FALSE) +           # overlay regression line
  xlab('albedo (% solar reflectivity)') + ylab('temperature (farenheit)') +
  theme_bw()

ggsave('./pics/c15-pic2.png')
```

```{r, fig.height=3.5, fig.width=5, fig.align='center', echo=FALSE}
knitr::include_graphics('./pics/c15-pic3.png')
```


As it turns out, the CLT allows us to model the error terms of a regression model as *normally distributed* with mean 0 and standard deviation equal to the *residual standard error*. The residual standard error is assumed to be constant (so at any point on the regression line, the error has the same distribution). In the above plot, the distribution of the residuals is depicted in red at two points in the data. Below is the regression output for temperature on albedo:

```{r}
reg2 = lm(temperature ~ albedo, data = nycheat)
summary(reg2)
```

The third line from the bottom shows the residual standard error of this model: 3.984. Using this we can generate a 95\% *prediction interval* for the predicted value of temperature given albedo. You can either do this manually (using the standard expression for a confidence interval) or simply use the `predict()` function in R.  

E.g. below is a 95\% prediction interval for the value of temperature when albedo = 10: 

```{r}
predict(reg2, newdata = data.frame(albedo=10), interval = 'predict', level = 0.95)
```

Note `fit` refers to the fitted value of the model (i.e. the point-estimate of the model prediction).  



\ 

# 16--2 Hypothesis tests on regression coefficients

We can use the hypothesis testing framework to test whether the predictor variables in a given model are really significant. Consider the multiple regression model of temperature, using using vegetation, albedo, building height, and population density as predictors:

```{r}
reg3 = lm(temperature ~ vegetation + albedo + building_height + pop_density, data = nycheat)
summary(reg3)
```

Note that each estimated coefficient also has an associated $p$-value (the column `Pr(>|t|)`). In R (and other software) the regression coefficients are treated as hypothesis tests, with the null hypothesis that the true coefficient is zero, i.e. 

$$H_0: \beta = 0$$ 
$$H_1: \beta \neq 0$$

The $p$-value represents the likelihood of getting a value as extreme as the observed coefficient if the null were true (i.e. if the coefficient were really zero). In the output above, the first three predictors have very low $p$-values, suggesting that the coefficients for those variables are significant. Note $p$-value for population density is larger than the others, at 0.02, and is thus significant only at the 5\% level.   

## Correcting for Correlation: the $F$-test

`coming soon`

\ 

## ANOVA 

`coming soon` 



\ 

# 16--3 Goodness-of-Fit with $R^2$ 

`coming soon`



\ 

# 16--4 Diagnostic Plots 

`coming soon`

## Checking the residuals 

## Checking for outliers 



\ 

# 16--5 Diagnosing a Regression Model 

`coming soon`

## Checking for multicollinearity 

## Checking for nonlinear relationships




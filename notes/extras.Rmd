---
title: 'extras'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 14--3 Multivariate Normal 

We introduced the normal distribution in chapter 9, as the continuous probability distribution with the density function:


A single distribution of data can be modeled using the univariate normal distribution, as given in the equation above.  

When we consider the joint variability of two variables, we can think of them as giving rise to a multivariate normal, i.e. a 3d gaussian:



The multivariate Gaussian has mean vector mu and correlation matrix Sigma.   

\ 

# 13--5 The t-test 

To recap: the $t$-distribution is used to approximate the limiting behavior of a sample mean, when the true mean and variance of a population is *unknown*. The $t$-distribution has one parameter, *degrees of freedom*, which describes the exact shape of the bell curve. For a sample of size $n$, the random variable $\frac{\bar X - \mu}{s/\sqrt n}$ follows a $t$-distribution with $n-1$ degrees of freedom. For more on this, see: <a href="https://stats103.org/notes/c11-clt.html#the-t-distribution">$t$-distribution</a>.  

Below are some common examples of hypothesis tests conducted using the $t$-distribution.  

## One-sample t-test

A one-sample $t$-test can be used to test whether the mean of a population is equal to some specified value. For a two-sided test, the null and alternative hypotheses are:

$$H_0: \mu = k$$
$$H_1: \mu \neq k$$

Using data with sample mean $\bar X$, sample s.d. $s$, and sample size $n$, the $t$-statistic for this test is:

$$t = \frac{\bar X - k}{\frac{s}{\sqrt n}}$$

For a two-sided test of size $\alpha$, the bounds of the rejection region are:

$$\bigg\{ k - t_{(1-\alpha/2)} \cdot \frac{s}{\sqrt n} \; , \; k + t_{(1-\alpha/2)} \cdot \frac{s}{\sqrt n} \bigg\}$$

To perform the test you can either compute these bounds manually, or you can use the `t.test()` function, entering the array of sample data as one argument, and the proposed null value of the mean as the second argument. E.g. for the pay gap data, testing whether the true mean bonus betweeen females and males is really zero:

```{r}
t.test(paygap$DiffMeanBonusPercent, mu = 0)
```

## Welch two-sample t-test 

The two-sample $t$-test can be used to test whether two groups have the same mean. The easiest way to do this is to think of the two groups as independent RVs, $X$ and $Y$, and to create a new RV that is a linear combination of both. For a two-sided test, the null and alternative hypotheses are:

$$H_0: \mu_X - \mu_Y = 0$$
$$H_1: \mu_X - \mu_Y \neq 0$$

where $\mu_X$ denotes the true mean of group $X$ and $\mu_Y$ denotes the true mean of group $Y$. 

The $t$-statistic for the test is:

$$t = \frac{\bar X - \bar Y}{\sqrt{\frac{s_X^2}{n_X}+\frac{s_Y^2}{n_Y}}}$$

where the subscripts $X$ and $Y$ denote the sample parameters for each of the two groups. In this case the distribution of the $t$-statistic follows a $t$-distribution with degrees of freedom:

$$\text{DoF} = \frac{\bigg( \frac{s_X^2}{n_X} + \frac{s_Y^2}{n_Y} \bigg)^2}{\frac{(s_X^2 / n_X)^2}{n_X-1}+\frac{(s_Y^2 / n_Y)^2}{n_Y-1}}$$

which is known as the Welch-Satterthwaite equation. For simplicity, you can use the smaller of $n_X-1$ and $n_Y-1$ as the DoF.  

In any case, the `t.test()` function in R will do these cumbersome calculations for you: 

```{r}
t.test(paygap$FemaleBonusPercent, paygap$MaleBonusPercent)
```

## Two-sample t-test with pooled data

If the two groups under scrutiny have the same population variance (or you have good reason to believe that they do), the difference in their sample means can be better approximated using what's known as the *pooled standard deviation:*

$$s_p = \sqrt{\frac{s_X^2 (n_X - 1)+s_Y^2(n_Y-1)}{n_X+n_Y-2}}$$

The $t$-statistic for this test is:

$$t = \frac{\bar X - \bar Y}{s_p \cdot \sqrt{\frac{1}{n_X}+\frac{1}{n_Y}}}$$

where the $t$-statistic follows a $t$-distribution with $n_X + n_Y - 2$ degrees of freedom. 

Pooling the standard deviation gives an unbiased estimate of the common variance of the two groups, which gives a more accurate model overall. Note that if the two groups don't have the same variance, the Welch two-sample test must be used instead.  

## Two-sample t-test with paired data

Two samples are said to be *paired* if the observations are matched, i.e. both samples have observations on the *same* subjects. In this case we can use the *paired difference test*, which has a $t$-statistic:

$$t = \frac{\bar X_D - \mu}{\frac{s_D}{\sqrt{n}}}$$

where the subscript $D$ denotes the sample parameter pertaining to the difference in measurements for each subject---e.g. $s_D$ is the s.d. of the *difference* in measurements for each subject between the two groups. The $t$-statistic in this case follows a $t$-distribution with $n-1$ degrees of freeedom.  

Using the paired $t$-test increases the overall power of the test (when compared to the unpaired Welch test), and so is beneficial in applicable contexts.  

In R you can perform a paired two-sample $t$ test by specifying the parameter `paired = TRUE` in the `t.test()` function.  



\ 

# 13--6 Pearson's $\chi^2$ Test 

The chi-squared test is useful for testing whether groups of categorical resemble each other. Consider the following example, which shows data on blood type and the observed incidence of a particular ailment: 

```{r, echo=FALSE}
ailment_study = as.table(rbind(c(55,50,7,24), c(7,5,3,13), c(26,32,8,17)))
dimnames(ailment_study) = list(' ' = c('no ailment','advanced ailment','minimal ailment'), 
                           ' ' = c('O','A','AB','B'))

kable(ailment_study)
```

Suppose we're interested in finding whether the strength of the ailment is dependent on blood type. In this test the null hypothesis is that the strength of the ailment is *not* dependent on blood type.^[If the null were true, and the strength of the ailment is independent of blood type, then the data for each of the categories should *resemble* each other.]   

To perform this test, for each cell in the data we compute the squared difference between the observed count and the expected count, divided by the expected count:

$$\frac{(\text{observed-expected}^2)}{\text{expected}}$$

E.g. for the first cell in the table (the number of subjects with no ailment and blood type O), the observed count is 55 and the expected count is 48.45.^[To calculate the expected count, note that there are 88 subjects in total with blood type O, and 247 subjects in the dataset in total, meaning the proportion of subjects in the study with blood type O is $\frac{88}{247} = 0.356$. Note also there are 136 subjects in in total with no ailment. This the *expected* count for subjects with no ailment and blood type O is $136 \cdot \frac{88}{246}=48.45$.] Thus: 

$$\frac{(55-48.45)^2}{48.45}$$

The chi-squared test-statistic is the sum of these values for each cell in the table:

$$\chi^2 = \sum_i^k \frac{(X_i - \E[X_i])^2}{\E[X_i]}$$

Which, in this case, is:

$$\Longrightarrow \;\; \chi^2 = 15.797$$

## The $\chi^2$ distribution 

The chi-squared test statistic follows the *chi-squared distribution*, which describes the distribution of a *sum* of several independent standard normal distributions. The chi-squared distribution has one parameter, degrees of freedom, calculated as:

$$\text{DoF} = (\text{num. rows - 1})(\text{num. cols - 1})$$

In the example above, DoF = $(3-1)(4-1) = 6$. Below is a plot showing the chi-squared distribution for different DoFs:

```{r, echo=FALSE, fig.width=7, fig.height=3.5, fig.align='center'}
curve(dchisq(x,1),xlim=c(0,15),ylim=c(0,0.6),ylab="Chi Square Density")
curve(dchisq(x,2),col="red",add=TRUE)
curve(dchisq(x,3),col="blue",add=TRUE)
curve(dchisq(x,5),col="dark green",add=TRUE)
curve(dchisq(x,10),col="brown",add=TRUE)
legend(12,0.55,c("DoF=1","DoF=2","DoF=3","DoF=5","DoF=10"),
       col=c("black","red","blue","dark green","brown"),lty=1:5)
```


## The $\chi^2$-test

In the example above, the observed test statistic $\chi^2 = 15.797$ encloses the following region in a $\chi^2$ distribution with 6 DoFs:

```{r, echo=FALSE, warning=FALSE, fig.width=5, fig.height=3.5, fig.align='center'}
x = seq(0,30, length = 1000)
y = dchisq(x,6)

df = data.frame(x = x, y = y)

ggplot(data = df, aes(x = x, y = y)) + 
  geom_line() +
  geom_area(aes(x = ifelse(x > 15.797, x, 0)), fill='violetred', alpha=0.4) +
  geom_segment(x = 15.797, xend = 15.797, y = 0, yend = 0.05, color = 'violetred', linetype = 'dotted') +
  geom_text(x = 15.797, y =  0.06, label = TeX("$\\chi^2 = 15.797$"), color = 'violetred') +
  ggtitle(TeX('$\\chi^2$ distribution with 6 DoF')) +
  ylim(0,0.14) +
  theme_light()
```

The shaded region is the $p$-value of the test, since this region contains values at least as extreme as the observed test statistic.  

In R we can perform the test using the `chisq.test()` function, as follows:

`r margin_note("Note the $\\texttt{chisq.text()}$ function requires data to be in the matrix or table format.")`

```{r, warning=FALSE}
## make contingency table 
ailment_study = as.table(rbind(c(55,50,7,24), c(7,5,3,13), c(26,32,8,17)))

## perform chi-squared test
chisq.test(ailment_study)
```

i.e. the $p$-value of the test is 0.0149, which implies we can reject the null at the 5\% level, and conclude that the strength of the ailment is not independent of blood type.  

Note in the above example we tested several hypotheses simultaneously---we could also have tested each group  (advanced ailment and minimal ailment) *separately* against the control for dependence on blood type:

```{r, warning=FALSE}
## test advanced ailament group against control group
chisq.test(ailment_study[c(1,2), ])
```

```{r, warning=FALSE}
## test minimal ailament group against control group
chisq.test(ailment_study[c(1,3), ])
```

Note how each of these groups, when tested *separately* against the control group, yielded vastly different results: the advanced ailment group has a significant $p$-value of 0.00343, but the minimal ailment group does not have a significant $p$-value.   



\ 

# Multiple Testing 

Testing many hypotheses simultaneously is known as *multiple testing*. For any one test, the chance of a false rejection of the null (type I error) is $\alpha$. But when conducting the tests  simultaneously, the chance of *at least* one false rejection is much higher.  This is the **multiple testing problem**.  




\ 

\ 

---

\ 





---
title: 'extras'
output:
  tufte::tufte_html:
    tufte_features: ['fonts','background','italics']
    css: '../mytufte.css'
    toc: true
    toc_depth: 2
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(extraDistr)
library(gridExtra)
library(latex2exp)
library(moments)
library(bookdown)
library(rsconnect)
library(tufte)
```

```{r, include=FALSE}
paygap <- read.csv('./data/gender-paygap-2019.csv')
paygap <- paygap %>%
  mutate(EmployerSize = factor(EmployerSize, levels = c('0-249','250-499','500-999','1000-4999','5000-19999','20000+')))
nycheat <- read.csv('./data/nyc-heatwave.csv')
```

\newcommand{\E}{\text{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\SD}{\text{SD}}
\newcommand{\SE}{\text{SE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\renewcommand{\P}{\text{P}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\sumin}{\sum_i^n}
\newcommand{\Bias}{\text{Bias}}

---

# 14--3 Multivariate Normal 

We introduced the normal distribution in chapter 9, as the continuous probability distribution with the density function:


A single distribution of data can be modeled using the univariate normal distribution, as given in the equation above.  

When we consider the joint variability of two variables, we can think of them as giving rise to a multivariate normal, i.e. a 3d gaussian:



The multivariate Gaussian has mean vector mu and correlation matrix Sigma.   

\ 




## The BH method 



by arranging the $p$-values $p_1, ..., p_m$ in *increasing* order, then for a given $\alpha$, finding the largest $k$ such that $p_k \leq k \frac \alpha m$. We then reject all null hypotheses $H_{0i}$ for $i = 1,...,k$.  

Some definitions:

- the false discovery rate (FDR) is the number of false rejections divided by the number of rejections 
- the false discovery proportion (FDP) is the proportion of false rejections
- given $m$ tests, let $m_0$ be the number of null hypotheses that are *true*
- let $m_1$ be the number of null hypotheses that are false, such that $m_1 = m - m_0$

Construct the following table showing the outcomes:

```{r, echo=FALSE}
multtests = as.table(rbind(c('$U$','$V$','$m_0$'), c('$T$','$S$','$m_1$'), c('$m-R$','$R$','$m$')))
dimnames(multtests) = list(' ' = c('$H_0$ true','$H_0$ false','total'), 
                           ' ' = c('$H_0$ not rejected','$H_0$ rejected','total'))

kable(multtests)
```

Using this table, define the FDP as follows:

$$\text{FDP} = \begin{cases} V/R & R > 0 \\ 0 & R=0 \end{cases}$$

Then, if $p_1, ..., p_m$ are the **ordered** $p$-values from the $m$ tests, define:

$$k_i = \frac{i}{C_m} \frac \alpha m \hspace{1cm} R = \text{max} \big\{ p_i < k_i \big\}$$

where $C_m = 1$ if the $p$-values are independent, and if not, then $C_m = \sum_i^m \frac 1i$. If we let $T = p_R$, then $T$ is the **BH rejection threshold**. We can thus reject all null hypotheses $H_{0i}$ for which $P_i \leq T$.   





\ 

\ 

---

\ 




